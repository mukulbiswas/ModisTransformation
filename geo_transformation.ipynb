{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion of MODIS File Format by Spherical Coordinates\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This program processes an HDF file and extract the data of a SDS of your choice. After having loaded the data file, it lists all the SDS available in it. You can pick the one of your interest and proceed.\n",
    "\n",
    "The notebook also gives you a chance to look into the data, remove the fill values, includes a function to output the values of a coordinate from signed decision to degree/minutes/seconds format ( *_you might not even need it_*).\n",
    "\n",
    "**Ensure when running this notebook**\n",
    "* You have Anaconda and Python installed on your computer.\n",
    "* You launch Jupyter NOT from the start-menu but from an Anaconda Prompt terminal.\n",
    "* You launch the Jupyter notebook from within the folder where it located -\n",
    "    - change directory in Anaconda Prompt using **cd** commands until you are where the *.ipnyb* file is located\n",
    "    - the data file is located right there or in a folder called *data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Note:_\n",
    "<i>\n",
    "1. Every block of code in this notebook is logically grouped\n",
    "2. The purpose of each block is briefed just before the block itself\n",
    "3. The clarity of the code and other technical explanations are included in the code as comments\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries\n",
    "Import all the necessary libraries that would be required for this project.\n",
    "SD is the library that is capable of handling HDF files generated by MODIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "\n",
    "from pyhdf import SD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above piece of code fail that would mean that you do not have all the libraries propoerly installed. Check the following: \n",
    "* The library itself was never installed. Install them using conda install command.\n",
    "* You have installed the libraries but they are not picked up by Jupyter environment. Fix your sys.path and kernal.\n",
    "* You did not launch Jupyter using 'jupyter notebook' command from within the folder where this notebook is located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the HDF File\n",
    "- Insert the filename in the code below that needs processing. Ensure the relative path of the file from the location of this notebook.\n",
    "- Currently this code is capable of loading one file at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your data file name between the quotes in the statement below. \n",
    "# Ensure that the file is located in a directory called 'data'\n",
    "\n",
    "try:\n",
    "    FILE_NAME = 'MYD04_L2.A2017249.2105.006.2017250160535.hdf'\n",
    "    hdf = SD.SD('../data/{}'.format(FILE_NAME))\n",
    "\n",
    "except:\n",
    "    print('There is a problem with the file format. Can you ensure that the HDF file that you have loaded is not corrupt.')\n",
    "    print('Try loading the HDF file using Panoply, if needed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latitudes and Longitudes\n",
    "- The complete set of latitudes and longitudes for all the data points are available within the HDF file. They can be extracted in 2 different arrays.\n",
    "- Determine the start and final lat-long and check if they are right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31.052055, -138.71803], [52.295116, -105.552605]]\n"
     ]
    }
   ],
   "source": [
    "# Find out the starting and ending lat-long for this data set\n",
    "lat = hdf.select('Latitude')\n",
    "lon = hdf.select('Longitude')\n",
    "    \n",
    "latitudes = lat[:]\n",
    "longitudes = lon[:]\n",
    "\n",
    "edgeCoordinates = [[latitudes.min(),  longitudes.min()], [latitudes.max(), longitudes.max()]]\n",
    "\n",
    "print(edgeCoordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset List\n",
    "- The HDF file may contain a number of datasets. It is best to list them down so that you can pick the right one.\n",
    "- Alternatively, load this HDF file on PanoPly and locate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Longitude\n",
      "1. Latitude\n",
      "2. Scan_Start_Time\n",
      "3. Solar_Zenith\n",
      "4. Solar_Azimuth\n",
      "5. Sensor_Zenith\n",
      "6. Sensor_Azimuth\n",
      "7. Scattering_Angle\n",
      "8. Land_sea_Flag\n",
      "9. Aerosol_Cldmask_Land_Ocean\n",
      "10. Cloud_Pixel_Distance_Land_Ocean\n",
      "11. Land_Ocean_Quality_Flag\n",
      "12. Optical_Depth_Land_And_Ocean\n",
      "13. Image_Optical_Depth_Land_And_Ocean\n",
      "14. Average_Cloud_Pixel_Distance_Land_Ocean\n",
      "15. Aerosol_Type_Land\n",
      "16. Fitting_Error_Land\n",
      "17. Surface_Reflectance_Land\n",
      "18. Corrected_Optical_Depth_Land\n",
      "19. Corrected_Optical_Depth_Land_wav2p1\n",
      "20. Optical_Depth_Ratio_Small_Land\n",
      "21. Number_Pixels_Used_Land\n",
      "22. Mean_Reflectance_Land\n",
      "23. STD_Reflectance_Land\n",
      "24. Mass_Concentration_Land\n",
      "25. Aerosol_Cloud_Fraction_Land\n",
      "26. Quality_Assurance_Land\n",
      "27. Solution_Index_Ocean_Small\n",
      "28. Solution_Index_Ocean_Large\n",
      "29. Effective_Optical_Depth_Best_Ocean\n",
      "30. Effective_Optical_Depth_Average_Ocean\n",
      "31. Optical_Depth_Small_Best_Ocean\n",
      "32. Optical_Depth_Small_Average_Ocean\n",
      "33. Optical_Depth_Large_Best_Ocean\n",
      "34. Optical_Depth_Large_Average_Ocean\n",
      "35. Mass_Concentration_Ocean\n",
      "36. Aerosol_Cloud_Fraction_Ocean\n",
      "37. Effective_Radius_Ocean\n",
      "38. PSML003_Ocean\n",
      "39. Asymmetry_Factor_Best_Ocean\n",
      "40. Asymmetry_Factor_Average_Ocean\n",
      "41. Backscattering_Ratio_Best_Ocean\n",
      "42. Backscattering_Ratio_Average_Ocean\n",
      "43. Angstrom_Exponent_1_Ocean\n",
      "44. Angstrom_Exponent_2_Ocean\n",
      "45. Least_Squares_Error_Ocean\n",
      "46. Optical_Depth_Ratio_Small_Ocean_0.55micron\n",
      "47. Optical_Depth_by_models_ocean\n",
      "48. Number_Pixels_Used_Ocean\n",
      "49. Mean_Reflectance_Ocean\n",
      "50. STD_Reflectance_Ocean\n",
      "51. Quality_Assurance_Ocean\n",
      "52. Deep_Blue_Aerosol_Optical_Depth_550_Land\n",
      "53. Deep_Blue_Spectral_Aerosol_Optical_Depth_Land\n",
      "54. Deep_Blue_Angstrom_Exponent_Land\n",
      "55. Deep_Blue_Spectral_Single_Scattering_Albedo_Land\n",
      "56. Deep_Blue_Spectral_Surface_Reflectance_Land\n",
      "57. Deep_Blue_Spectral_TOA_Reflectance_Land\n",
      "58. Deep_Blue_Number_Pixels_Used_550_Land\n",
      "59. Deep_Blue_Aerosol_Optical_Depth_550_Land_STD\n",
      "60. Deep_Blue_Cloud_Fraction_Land\n",
      "61. Deep_Blue_Aerosol_Optical_Depth_550_Land_QA_Flag\n",
      "62. Deep_Blue_Algorithm_Flag_Land\n",
      "63. Deep_Blue_Aerosol_Optical_Depth_550_Land_Best_Estimate\n",
      "64. Deep_Blue_Aerosol_Optical_Depth_550_Land_Estimated_Uncertainty\n",
      "65. AOD_550_Dark_Target_Deep_Blue_Combined\n",
      "66. AOD_550_Dark_Target_Deep_Blue_Combined_QA_Flag\n",
      "67. AOD_550_Dark_Target_Deep_Blue_Combined_Algorithm_Flag\n",
      "68. Glint_Angle\n",
      "69. Wind_Speed_Ncep_Ocean\n",
      "70. Topographic_Altitude_Land\n",
      "71. Effective_Optical_Depth_0p55um_Ocean\n"
     ]
    }
   ],
   "source": [
    "datasets = hdf.datasets()\n",
    "sdsList = []\n",
    "\n",
    "for i, v in enumerate(datasets):\n",
    "    sdsList += [v]    # extend, not append\n",
    "    print('{}. {}'.format(i, v))\n",
    "\n",
    "sdsCount = i + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the Data\n",
    "- Replace the name of the dataset below to the one of your interest.\n",
    "- The dataset itself could either be 2 dimensional or be 3-dimensional. The below code is generalised for the kinds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows 203 and no of columns 135\n",
      "Also, there are 3 layers of data available for each lat-long\n"
     ]
    }
   ],
   "source": [
    "sds = hdf.select('Corrected_Optical_Depth_Land')\n",
    "data = sds.get()\n",
    "\n",
    "# If the data is 3D then the size of outermost list is the 'depth'; in case of 2D, it is the row\n",
    "if data.ndim == 2:\n",
    "    nRows = len(data)\n",
    "    nColumns = len(data[0])\n",
    "else:\n",
    "    nDepth = len(data)\n",
    "    nRows = len(data[0])\n",
    "    nColumns = len(data[0][0])\n",
    "               \n",
    "print('No of rows {} and no of columns {}'.format(nRows, nColumns))\n",
    "if data.ndim == 3:\n",
    "    print('Also, there are {} layers of data available for each lat-long'.format(nDepth))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the Data\n",
    "- Replace the fill values\n",
    "- Apply the scale factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scale factor of this data is 0.001.\n",
      "The top 10 rows of your raw data look like this. It shows only the first 10 of the total 135 columns.\n",
      "[[[0.25900001 0.27600001 0.28400001 0.37700002 0.49200002        nan\n",
      "          nan        nan        nan        nan]\n",
      "  [0.23200001 0.19000001 0.25600001 0.33000002 0.44700002        nan\n",
      "          nan        nan        nan        nan]\n",
      "  [0.22400001 0.15800001 0.24500001 0.34100002        nan        nan\n",
      "          nan        nan 0.44800002 0.42100002]\n",
      "  [0.27200001 0.15600001 0.22100001 0.34300002        nan        nan\n",
      "   0.42300002        nan 0.41200002 0.40700002]\n",
      "  [0.28100001 0.13300001 0.20300001 0.27900001        nan        nan\n",
      "          nan 0.42000002 0.33400002 0.36200002]\n",
      "  [0.28100001 0.15500001 0.24600001 0.28300001 0.38600002        nan\n",
      "          nan 0.43000002 0.30800001 0.35400002]\n",
      "  [0.27400001 0.20600001 0.23900001 0.26200001 0.37600002        nan\n",
      "          nan        nan 0.30700001 0.32500002]\n",
      "  [0.28000001 0.22400001 0.33400002 0.30800001        nan        nan\n",
      "          nan        nan 0.34900002 0.28600001]\n",
      "  [0.27900001 0.24600001 0.38400002 0.41000002        nan        nan\n",
      "   0.43900002 0.31200001 0.31400001 0.25200001]\n",
      "  [0.27200001 0.29100001 0.32900002 0.47300002        nan        nan\n",
      "          nan 0.30800001 0.18500001 0.25100001]]\n",
      "\n",
      " [[0.23100001 0.20400001 0.21300001 0.30900001 0.40200002        nan\n",
      "          nan        nan        nan        nan]\n",
      "  [0.18900001 0.15500001 0.18900001 0.27000001 0.36600002        nan\n",
      "          nan        nan        nan        nan]\n",
      "  [0.18300001 0.12900001 0.18100001 0.28000001        nan        nan\n",
      "          nan        nan 0.36700002 0.34500002]\n",
      "  [0.20200001 0.11500001 0.16300001 0.28100001        nan        nan\n",
      "   0.34600002        nan 0.33700002 0.33300002]\n",
      "  [0.20900001 0.098      0.17400001 0.22800001        nan        nan\n",
      "          nan 0.31200001 0.27300001 0.29600001]\n",
      "  [0.20900001 0.11400001 0.22000001 0.23200001 0.31600002        nan\n",
      "          nan 0.32000002 0.25200001 0.29000001]\n",
      "  [0.20400001 0.15200001 0.21300001 0.21500001 0.30800001        nan\n",
      "          nan        nan 0.25100001 0.26600001]\n",
      "  [0.20800001 0.16500001 0.29900001 0.25200001        nan        nan\n",
      "          nan        nan 0.28500001 0.23400001]\n",
      "  [0.20700001 0.18100001 0.34400002 0.33500002        nan        nan\n",
      "   0.33700002 0.23000001 0.23200001 0.20600001]\n",
      "  [0.20300001 0.21500001 0.29500001 0.38700002        nan        nan\n",
      "          nan 0.22700001 0.15200001 0.20500001]]\n",
      "\n",
      " [[0.21100001 0.15300001 0.16300001        nan        nan        nan\n",
      "          nan        nan        nan        nan]\n",
      "  [       nan        nan 0.14200001        nan        nan        nan\n",
      "          nan        nan        nan        nan]\n",
      "  [       nan        nan 0.13600001        nan        nan        nan\n",
      "          nan        nan        nan        nan]\n",
      "  [0.15200001 0.086      0.12200001        nan        nan        nan\n",
      "          nan        nan        nan        nan]\n",
      "  [0.15700001 0.073      0.15300001        nan        nan        nan\n",
      "          nan 0.23500001        nan        nan]\n",
      "  [0.15700001 0.086      0.20100001        nan        nan        nan\n",
      "          nan 0.24100001        nan        nan]\n",
      "  [0.15300001 0.11400001 0.19500001        nan        nan        nan\n",
      "          nan        nan        nan        nan]\n",
      "  [0.15600001 0.12400001 0.27400001        nan        nan        nan\n",
      "          nan        nan        nan        nan]\n",
      "  [0.15600001 0.13600001 0.31600002        nan        nan        nan\n",
      "   0.26400001 0.17300001 0.17400001        nan]\n",
      "  [0.15200001 0.16100001 0.26900001        nan        nan        nan\n",
      "          nan 0.17100001        nan        nan]]]\n"
     ]
    }
   ],
   "source": [
    "# Replace the fill-values (the missing data points) with NaN\n",
    "attrs = sds.attributes(full=1)\n",
    "fillvalue=attrs['_FillValue']\n",
    "fv = fillvalue[0]\n",
    "\n",
    "# Turn fillvalues to NaN\n",
    "data=data.astype(float)\n",
    "data[data == fv] = np.nan\n",
    "\n",
    "# Apply the scaleFactor\n",
    "attributes=sds.attributes()\n",
    "scaleFactor=attributes['scale_factor']\n",
    "print('The scale factor of this data is {:.3f}.'.format(scaleFactor))\n",
    "\n",
    "scaledData = data * scaleFactor\n",
    "\n",
    "print('The top 10 rows of your raw data look like this. It shows only the first 10 of the total {} columns.'.format(nColumns))\n",
    "print(scaledData[0:10, 0:10] if scaledData.ndim == 2 else scaledData[:, 0:10, 0:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop the Data\n",
    "- The downloaded data could be much more than what you might be interested in. Enter the start and final lat-long coordinates.\n",
    "- If you ignore this step then the entire dataset will be processed and exported to the final output file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0, -125.0 - 50.0, -115.0\n"
     ]
    }
   ],
   "source": [
    "# Please uncomment and enter the new start lat-long, separated by a space (e.g., 23.208 88.762).\n",
    "\n",
    "edgeCoordinates[0] = [ 45.0, -125.0 ]\n",
    "\n",
    "lat1 = float(edgeCoordinates[0][0])\n",
    "lon1 = float(edgeCoordinates[0][1])\n",
    "\n",
    "# Please uncomment enter the new final lat-long, separated by a space (e.g., 35.812 92.253)\n",
    "\n",
    "edgeCoordinates[1] = [ 50.0, -115.0 ]\n",
    "\n",
    "lat2 = float(edgeCoordinates[1][0])\n",
    "lon2 = float(edgeCoordinates[1][1])\n",
    "\n",
    "\n",
    "print(\"{}, {} - {}, {}\".format(lat1, lon1, lat2, lon2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Format\n",
    "- The datapoints will be inserted into an array with each updated with its latitude and longitude values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45.042225, -115.47438, 1.9620000931899995, 1.5830000751884654, 1.2790000607492402], [45.025204, -115.6595, 2.1410001016920432, 1.7400000826455653, 1.4140000671613961], [45.007553, -115.84765, 2.6220001245383173, 2.145000101882033, 1.7530000832630321], [45.130497, -115.49281, 1.7650000838330016, 1.4080000668764114, 1.1250000534346327], [45.112797, -115.68523, 3.011000143014826, 2.462000116938725, 2.012000095564872], [45.09523, -115.87281, 2.4340001156087965, 1.9910000945674255, 1.6270000772783533], [45.078484, -116.048904, 2.499000118696131, 2.0440000970847905, 1.6710000793682411], [45.06124, -116.22701, 2.331000110716559, 1.9070000905776396, 1.5590000740485266], [45.044327, -116.399055, 2.4360001157037914, 1.9930000946624205, 1.6290000773733482], [45.02709, -116.57148, 2.1000000997446477, 1.7030000808881596, 1.387000065878965]]\n"
     ]
    }
   ],
   "source": [
    "outputTable = []\n",
    "\n",
    "if lat1 < lat2:\n",
    "    minLat, maxLat = lat1, lat2\n",
    "else:\n",
    "    minLat, maxLat = lat2, lat1\n",
    "\n",
    "if lon1 < lon2:\n",
    "    minLon, maxLon = lon1, lon2\n",
    "else:\n",
    "    minLon, maxLon = lon2, lon1\n",
    "\n",
    "if scaledData.ndim == 2:\n",
    "    for i in range(nRows):\n",
    "        for j in range(nColumns):\n",
    "            if latitudes[i,j] >= minLat and latitudes[i,j] <= maxLat:\n",
    "                if longitudes[i,j] >= minLon and longitudes[i,j] <= maxLon:\n",
    "                    outputTable.append([latitudes[i,j], longitudes[i,j], scaledData[i,j]])\n",
    "else:\n",
    "    # tempList = []\n",
    "    for i in range(nRows):\n",
    "        for j in range(nColumns):\n",
    "             if latitudes[i,j] >= minLat and latitudes[i,j] <= maxLat:\n",
    "                if longitudes[i,j] >= minLon and longitudes[i,j] <= maxLon:\n",
    "\n",
    "                    tempList = [latitudes[i,j], longitudes[i,j]]\n",
    "\n",
    "                    isRowNan = True # set to true but will become false if minimum one item in the row is a valid number\n",
    "                    for k in range(nDepth):\n",
    "                        value = scaledData[k, i, j]\n",
    "                        isRowNan = False if not np.isnan(value) else isRowNan\n",
    "                        tempList += [value]     # the original data comes with depth as the 1st level of hierarchy\n",
    "                    if not isRowNan:\n",
    "                        outputTable.append(tempList)\n",
    "\n",
    "print(outputTable[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save in an Output File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "OUTPUT_FILE = '../data/{}.csv'.format(FILE_NAME[:-4])\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', newline = '') as myfile:\n",
    "     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "     for line in outputTable:\n",
    "         wr.writerow(line)\n",
    "\n",
    "print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
